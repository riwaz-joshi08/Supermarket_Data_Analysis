{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3925206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fc6018",
   "metadata": {},
   "source": [
    "### Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59c0b8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_20428\\790930629.py:1: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  df=pd.read_csv('Data_Sets\\SuperMarket Analysis.csv')\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_20428\\790930629.py:1: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  df=pd.read_csv('Data_Sets\\SuperMarket Analysis.csv')\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data_Sets\\\\SuperMarket Analysis.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df=\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mData_Sets\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mSuperMarket Analysis.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\OneDrive\\Desktop\\Supermarket_Data_Analysis\\env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\OneDrive\\Desktop\\Supermarket_Data_Analysis\\env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\OneDrive\\Desktop\\Supermarket_Data_Analysis\\env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\OneDrive\\Desktop\\Supermarket_Data_Analysis\\env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\OneDrive\\Desktop\\Supermarket_Data_Analysis\\env\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Data_Sets\\\\SuperMarket Analysis.csv'"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('Data_Sets\\SuperMarket Analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9b49fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c736144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c702cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8cea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab536eda",
   "metadata": {},
   "source": [
    "### Feature Engineering on Date/Time Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d368c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date']=pd.to_datetime(df['Date'])\n",
    "df['Time'] = pd.to_datetime(df['Time'], format='%I:%M:%S %p').dt.time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992525aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98b9c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Time'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e375427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602169e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Models cannot use raw date and time data directly do we convert them into numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f638800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbe0c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also map the values if necassary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0ba43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DayOfWeek'] = df['Date'].dt.dayofweek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d048e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Day'] = df['Date'].dt.day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d21ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Month'] = df['Date'].dt.month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f165b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Fot Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab891f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Hour'] = pd.to_datetime(df['Time'], format='%H:%M:%S').dt.hour\n",
    "df['Hour'] = df['Time'].apply(lambda x: x.hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e8fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TimeOfDay'] = df['Hour'].apply(\n",
    "    lambda h: 'Morning' if 5 <= h < 12 else\n",
    "              'Afternoon' if 12 <= h < 17 else\n",
    "              'Evening' if 17 <= h < 21 else\n",
    "              'Night'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e79659",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e6d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Date','Time'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29c8b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Invoice ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b49e6c3",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6adfe98",
   "metadata": {},
   "source": [
    "##### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20614755",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols=df.select_dtypes(include=['number','int','float'])\n",
    "cat_cols=df.select_dtypes(exclude=['number','int','float'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f027c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddbcbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "print(\"Skewness:\", skew(num_cols['Sales']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc545ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness 0.8912303920037631 means that the target column is right skewed, hence we use log transformation to make it normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20874a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will do this after initail model trianing IF NECESSARY\n",
    "# df['Sales_transformed'] = np.log1p(num_cols['Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b417fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import skew\n",
    "# print(skew(train_cleaned['Sales_transformed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5142129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using powertransformer to handle skeweness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c48e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip for now\n",
    "# from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# pt = PowerTransformer(method='yeo-johnson')\n",
    "# train_cleaned['Sales_transformed'] = pt.fit_transform(num_cols[['Sales']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec53c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(5,6))\n",
    "# sns.histplot(train_cleaned['Sales_transformed'], kde=True)\n",
    "# plt.title(f'Distribution of {col}')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dd5edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols=df.select_dtypes(include=['int','float','number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c75cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    plt.figure(figsize=(5,6))\n",
    "    sns.boxplot(num_cols[col])\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b3293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that there are outliers present in the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00848196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# Q1 = num_cols['Sales_transformed'].quantile(0.25)\n",
    "# Q3 = num_cols['Sales_transformed'].quantile(0.75)\n",
    "# IQR = Q3 - Q1\n",
    "\n",
    "# lower_bound = Q1 - 1.5 * IQR\n",
    "# upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# outliers = num_cols[(num_cols['Sales_transformed'] < lower_bound) | (num_cols['Sales_transformed'] > upper_bound)]\n",
    "# print(f\"Outlier count: {len(outliers)} ({len(outliers)/len(num_cols)*100:.2f}% of data)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To deal with outlier we use Winsorization\n",
    "\n",
    "# Winsorization is a statistical technique used to limit the influence of extreme values (outliers)\n",
    "#  by capping them at specific percentile thresholds — rather than removing them.\n",
    "# It brings extreme values closer to the bulk of the data, instead of deleting them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb834a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip for now\n",
    "# from scipy.stats.mstats import winsorize\n",
    "# train_cleaned['Sales_transformed_capped']=winsorize(train_cleaned['Sales_transformed'],limits=[0.01,0.01])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e80a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee9ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    vc = df[col].value_counts() # counts how many times each unique category appears in that column.\n",
    "    pct = df[col].value_counts(normalize=True).mul(100)#finds what percentage of total data each represents.\n",
    "    summary = pd.concat([vc, pct], axis=1)# axis=1 means to concatenate horizontally\n",
    "    summary.columns = ['count','percent']\n",
    "    print(f\"\\n=== {col} ===\")\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6bdec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca0852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be5179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in cat_cols:\n",
    "    sns.countplot(data=df,x=col)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e186706",
   "metadata": {},
   "source": [
    "-Bivariate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773de382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Numerical vs Numerical\n",
    "# -----------------------------\n",
    "# Scatterplots for all numeric column pairs\n",
    "num_columns = num_cols.columns\n",
    "\n",
    "for i in range(len(num_columns)):\n",
    "    for j in range(i+1, len(num_columns)):\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.scatterplot(x=df[num_columns[i]], y=df[num_columns[j]])\n",
    "        plt.title(f'{num_columns[i]} vs {num_columns[j]}')\n",
    "        plt.xlabel(num_columns[i])\n",
    "        plt.ylabel(num_columns[j])\n",
    "        plt.show()\n",
    "\n",
    "# Correlation Heatmap\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(df[num_columns].corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap for Numerical Variables\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Numerical vs Categorical\n",
    "# -----------------------------\n",
    "cat_columns = cat_cols.columns\n",
    "\n",
    "for cat in cat_columns:\n",
    "    for num in num_columns:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.boxplot(x=df[cat], y=df[num])\n",
    "        plt.title(f'{num} by {cat}')\n",
    "        plt.xlabel(cat)\n",
    "        plt.ylabel(num)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Categorical vs Categorical\n",
    "# -----------------------------\n",
    "for i in range(len(cat_columns)):\n",
    "    for j in range(i+1, len(cat_columns)):\n",
    "        ctab = pd.crosstab(df[cat_columns[i]], df[cat_columns[j]])\n",
    "\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.heatmap(ctab, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'{cat_columns[i]} vs {cat_columns[j]}')\n",
    "        plt.xlabel(cat_columns[j])\n",
    "        plt.ylabel(cat_columns[i])\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=45)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7111567a",
   "metadata": {},
   "source": [
    "Regression Line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8634097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assume train_processed has been preprocessed\n",
    "# numeric_cols = df.select_dtypes(include=['float64','int64']).columns\n",
    "\n",
    "# # Compute correlation matrix\n",
    "# corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "# #Regression line plot\n",
    "# sns.lmplot(x='Item_MRP', y='Sales_transformed_capped', data=df, height=6)\n",
    "# plt.title(\"Linear Regression Line: MRP vs Sales\")\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725ca427",
   "metadata": {},
   "source": [
    "Scatter Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8482ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8,6))\n",
    "# sns.scatterplot(x='Item_MRP', y='Sales_transformed_capped', data=train_cleaned)\n",
    "# plt.title(\"Item MRP vs Sales\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017fc765",
   "metadata": {},
   "source": [
    "Box plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445ec0b6",
   "metadata": {},
   "source": [
    "To see the sales in outlet location types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97d035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_cols = train_cleaned.select_dtypes(include=['object']).columns\n",
    "\n",
    "# # Example: How Outlet_Location_Type affects Sales\n",
    "# plt.figure(figsize=(8,6))\n",
    "# sns.boxplot(x='Outlet_Location_Type', y='Sales_transformed_capped', data=train_cleaned)\n",
    "# plt.title(\"Outlet Type vs Sales\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35055b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(5,6))\n",
    "# sns.barplot(x='Item_Type', y='Sales_transformed_capped', data=train_cleaned)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f550b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ae278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb7a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(num_cols.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap of Numerical Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8971b44",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5355e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Item Category\n",
    "# # Extract first two letters from Item_ID to classify into Food/Drinks/Non-Consumable\n",
    "\n",
    "# combined['Item_Category'] = combined['Item_ID'].str[:2].map({\n",
    "#     'FD': 'Food',\n",
    "#     'DR': 'Drinks',\n",
    "#     'NC': 'Non-Consumable'\n",
    "# })\n",
    "\n",
    "\n",
    "# # Interaction Feature\n",
    "# # IT refers to new features created by combining two or more existing features in a way that captures their relationship or interaction\n",
    "# # Numerical Columns\n",
    "# # 1. Relationship between item price and weight\n",
    "# combined['MRP_Weight'] = combined['Item_MRP'] * combined['Item_W']\n",
    "\n",
    "# #2.Item MRP × Sales:\n",
    "# combined['MRP_Sales_interaction'] = combined['Item_MRP'] * combined['Sales']\n",
    "\n",
    "# # 3. Item MRP × Outlet Year\n",
    "# combined['MRP_OutletYear_interaction'] = combined['Item_MRP'] * combined['Outlet_Year']\n",
    "\n",
    "# #4. Sales × MRP Weight\n",
    "\n",
    "# combined['Sales_MRPWeight_interaction']= combined['Sales'] * combined['MRP_Weight']\n",
    "\n",
    "# # Categorical Columns\n",
    "# # -------------------------------------------------------------------\n",
    "# #Research Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754db0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fitting on train and transforming test separately may be okay for label encoding,\n",
    "# # but doing all encodings on `combined` ensures absolute consistency for all categorical transforms.\n",
    "# # Label-encode 'Outlet_ID' on combined so encoding is consistent across train/test\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# combined['Outlet_ID'] = le.fit_transform(combined['Outlet_ID'].astype(str))\n",
    "\n",
    "# # Now that we've applied all encodings on `combined`, re-split into train_cleaned and test_cleaned\n",
    "# # This ensures the encoded columns appear exactly the same in both sets.\n",
    "# train_cleaned = combined[combined['source'] == 'train'].copy()\n",
    "# test_cleaned = combined[combined['source'] == 'test'].copy()\n",
    "\n",
    "# # If 'Sales_transformed_capped' was previously created for modeling, ensure it exists for train\n",
    "# # If not present yet, create it by copying 'Sales' (this was your previous simple approach)\n",
    "# if 'Sales_transformed_capped' not in train_cleaned.columns:\n",
    "#     train_cleaned['Sales_transformed_capped'] = train_cleaned['Sales']\n",
    "\n",
    "# # Drop the helper 'source' column now that we've re-split (keeps dataframes clean)\n",
    "# train_cleaned.drop(columns=['source'], inplace=True)\n",
    "# test_cleaned.drop(columns=['source'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6a80f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create output folder, fit the chosen model on the full training data, and save model + preprocessor\n",
    "# import os\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# os.makedirs('ML_Model', exist_ok=True)\n",
    "\n",
    "# # Instantiate the model\n",
    "# best_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# # Fit on the full preprocessed training data\n",
    "# # X_train_processed and y_train are expected to be in the notebook kernel\n",
    "# best_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# # Save the fitted model\n",
    "# import joblib\n",
    "# joblib.dump(best_model, os.path.join('ML_Model', 'GradientBoostingRegressor.pkl'))\n",
    "# print(\"Saved fitted GradientBoostingRegressor to ML_Model/GradientBoostingRegressor.pkl\")\n",
    "\n",
    "# # Also save the preprocessing pipeline so you can transform new data the same way\n",
    "# try:\n",
    "#     joblib.dump(preprocessor, os.path.join('ML_Model', 'preprocessor.pkl'))\n",
    "#     print(\"Saved preprocessor to ML_Model/preprocessor.pkl\")\n",
    "# except Exception as e:\n",
    "#     print(\"Warning: could not save preprocessor:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f122f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TARGET & DATA DIAGNOSTICS\n",
    "# print(\"=== Target (Sales) Diagnostics ===\")\n",
    "# print(\"Target statistics:\")\n",
    "# print(y_train.describe())\n",
    "# print(\"\\nTarget distribution skewness:\", y_train.skew())\n",
    "# print(\"\\nFeature count:\", X_train_processed.shape[1])\n",
    "# print(\"Train samples:\", X_train_processed.shape[0])\n",
    "# print(\"Target variance:\", y_train.var())\n",
    "# print(\"Target std dev:\", y_train.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3324ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if target has unusual properties\n",
    "# print(\"\\nSales value counts (bottom 10):\")\n",
    "# print(y_train.value_counts().tail(10))\n",
    "# print(\"\\nUnique sales values:\", y_train.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd2a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature importance via correlation matrix\n",
    "# print(\"\\n=== Feature Summary ===\")\n",
    "# print(\"Features that will be used:\")\n",
    "# for i, feat in enumerate(numeric_features):\n",
    "#     print(f\"  {i+1}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bc1623",
   "metadata": {},
   "source": [
    "### Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2812ce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0029b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding numerical columns with standard scalar\n",
    "num_cols = num_cols.columns.tolist()\n",
    "scaler = StandardScaler()\n",
    "df_scaled = df.copy()  # keep original dataset\n",
    "df_scaled[num_cols] = scaler.fit_transform(df[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3295aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_cols.drop(columns=[\"Invoice ID\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db037e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e80d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical columns with One Hpt Encoding\n",
    "#Note Customer Type might need to be encoded in another way\n",
    "cat_cols = cat_cols.columns.tolist()\n",
    "df_encoded = pd.get_dummies(df, columns=cat_cols, drop_first=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99038c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_encoded.drop(columns=[\"Invoice ID\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b5496",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4ad153",
   "metadata": {},
   "source": [
    "### Initial Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b01f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "X = df_encoded.drop(columns=[\"Sales\"])  # features\n",
    "y = df_encoded[\"Sales\"]                 # target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,     # 20% test, 80% train\n",
    "    random_state=42,   # reproducible\n",
    ")\n",
    "\n",
    "\n",
    "models =  {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(alpha=1.0),\n",
    "    \"Lasso Regression\": Lasso(alpha=0.1),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    \"SVR\": SVR(kernel=\"rbf\")\n",
    "}\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"=== {name} ===\")\n",
    "    print(\"R² Score:\", r2_score(y_test, y_pred))\n",
    "    print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "    print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a6d7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
